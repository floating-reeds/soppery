{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Zero-Shot Citation Hallucination Detection\n",
                "Full pipeline: **Generate → Verify → Score → Visualize**\n",
                "\n",
                "Adapts the FACTUM framework for zero-shot (non-RAG) citation hallucination detection.\n",
                "- **ICS** (Internal Consistency Score): cosine sim between citation hidden state and mean essay context\n",
                "- **POS** (Pathway Orthogonality Score): 1 - |cos(pre_layer, post_layer)| at citation token\n",
                "- **PFS** (Pathway Fluctuation Score): L2 norm of the hidden state update\n",
                "- **BAS** (BOS Attention Score): attention from citation token to BOS (position 0)\n",
                "\n",
                "> **Note:** FACTUM-main code is NOT used at runtime — all scoring is self-contained here."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 1. Setup"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "!pip install -q torch transformers accelerate requests tqdm\n",
                "import torch, gc\n",
                "assert torch.cuda.is_available(), 'Switch to GPU runtime!'\n",
                "print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
                "print(f\"VRAM: {torch.cuda.get_device_properties(0).total_mem / 1e9:.1f} GB\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from huggingface_hub import login\n",
                "login(token=\"\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "MODEL_NAME = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
                "MAX_PROMPTS = 5\n",
                "MAX_NEW_TOKENS = 1024\n",
                "TEMPERATURE = 0.7\n",
                "SCORE_MAX_LEN = 512  # Max tokens for scoring (fits T4 VRAM)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 2. Prompts"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "PROMPTS = [\n",
                "    {\"id\": \"sci_001\", \"domain\": \"scientific\", \"prompt\": \"Write a 500-word essay explaining the Transformer architecture and its impact on natural language processing. Include at least 3 citations to foundational papers with author names and publication years.\"},\n",
                "    {\"id\": \"sci_002\", \"domain\": \"scientific\", \"prompt\": \"Discuss the development and applications of CRISPR-Cas9 gene editing technology. Cite at least 3 seminal papers that contributed to this field.\"},\n",
                "    {\"id\": \"sci_003\", \"domain\": \"scientific\", \"prompt\": \"Explain how deep reinforcement learning achieved superhuman performance in games like Go and Chess. Reference the key papers with proper citations.\"},\n",
                "    {\"id\": \"sci_004\", \"domain\": \"scientific\", \"prompt\": \"Describe the methodology and significance of the AlphaFold protein structure prediction system. Include citations to the original research papers.\"},\n",
                "    {\"id\": \"sci_005\", \"domain\": \"scientific\", \"prompt\": \"Write about the discovery and implications of the Higgs boson. Cite the foundational theoretical papers and experimental confirmation studies.\"},\n",
                "]\n",
                "print(f\"{len(PROMPTS)} prompts loaded\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 3. Citation Extractor"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import re\n",
                "from dataclasses import dataclass, asdict, field\n",
                "from typing import List, Optional\n",
                "\n",
                "@dataclass\n",
                "class Citation:\n",
                "    raw_text: str\n",
                "    start_pos: int\n",
                "    end_pos: int\n",
                "    extracted_authors: Optional[List[str]] = None\n",
                "    extracted_year: Optional[int] = None\n",
                "    extracted_title: Optional[str] = None\n",
                "    citation_type: str = \"academic\"\n",
                "\n",
                "CITATION_PATTERNS = [\n",
                "    re.compile(r'\\(([A-Z][a-z]+(?:\\s+(?:et\\s+al\\.?|&|and)\\s+[A-Z][a-z]+)*,?\\s*\\d{4}[a-z]?)\\)'),\n",
                "    re.compile(r'([A-Z][a-z]+(?:\\s+et\\s+al\\.?))\\s*\\((\\d{4}[a-z]?)\\)'),\n",
                "    re.compile(r'([A-Z][a-z]+(?:\\s+(?:and|&)\\s+[A-Z][a-z]+)?)\\s*\\((\\d{4})\\)'),\n",
                "    re.compile(r'\"([^\"]{10,100})\"\\s*\\((\\d{4})\\)'),\n",
                "]\n",
                "\n",
                "def extract_citations(text):\n",
                "    citations, seen = [], set()\n",
                "    for pat in CITATION_PATTERNS:\n",
                "        for m in pat.finditer(text):\n",
                "            s, e = m.span()\n",
                "            if any(s <= p <= e for p in seen): continue\n",
                "            seen.update(range(s, e + 1))\n",
                "            c = Citation(raw_text=m.group(0), start_pos=s, end_pos=e)\n",
                "            yr = re.search(r'\\d{4}', m.group(0))\n",
                "            if yr: c.extracted_year = int(yr.group())\n",
                "            au = re.search(r'([A-Z][a-z]+)', m.group(0))\n",
                "            if au: c.extracted_authors = [au.group(1)]\n",
                "            citations.append(c)\n",
                "    return citations\n",
                "\n",
                "# Quick test\n",
                "test = extract_citations('Vaswani et al. (2017) introduced Transformers. See also (Devlin et al., 2019).')\n",
                "print(f'Test: {len(test)} citations — {\", \".join(c.raw_text for c in test)}')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 4. Generate Essays"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
                "from tqdm import tqdm\n",
                "import json\n",
                "\n",
                "print(f'Loading {MODEL_NAME}...')\n",
                "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
                "model = AutoModelForCausalLM.from_pretrained(MODEL_NAME, torch_dtype=torch.float16, device_map='auto')\n",
                "if tokenizer.pad_token is None: tokenizer.pad_token = tokenizer.eos_token\n",
                "print(f'Loaded. {model.config.num_hidden_layers}L, {model.config.num_attention_heads}H')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "SYSTEM_PROMPT = \"\"\"You are an academic expert writing for a peer-reviewed journal.\n",
                "You MUST include proper academic citations in the format: Author (Year) or (Author et al., Year).\n",
                "Every factual claim must be supported by a citation. Use real author names and years.\n",
                "Example: Vaswani et al. (2017) introduced the Transformer architecture.\"\"\"\n",
                "\n",
                "essays = []\n",
                "for p in tqdm(PROMPTS[:MAX_PROMPTS], desc='Generating'):\n",
                "    messages = [{\"role\": \"system\", \"content\": SYSTEM_PROMPT}, {\"role\": \"user\", \"content\": p[\"prompt\"]}]\n",
                "    try:\n",
                "        formatted = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
                "    except Exception:\n",
                "        formatted = f\"{SYSTEM_PROMPT}\\n\\nRequest: {p['prompt']}\\n\\nResponse:\"\n",
                "\n",
                "    inputs = tokenizer(formatted, return_tensors='pt', truncation=True, max_length=2048).to('cuda')\n",
                "    with torch.no_grad():\n",
                "        out = model.generate(**inputs, max_new_tokens=MAX_NEW_TOKENS, temperature=TEMPERATURE,\n",
                "                             top_p=0.9, do_sample=True, pad_token_id=tokenizer.pad_token_id)\n",
                "    response = tokenizer.decode(out[0][inputs.input_ids.shape[1]:], skip_special_tokens=True).strip()\n",
                "    cites = extract_citations(response)\n",
                "    essays.append({'prompt_id': p['id'], 'domain': p['domain'], 'prompt': p['prompt'],\n",
                "                   'model_name': MODEL_NAME, 'response': response, 'citations': [asdict(c) for c in cites]})\n",
                "    print(f\"  {p['id']}: {len(response)} chars, {len(cites)} citations\")\n",
                "\n",
                "total_c = sum(len(e['citations']) for e in essays)\n",
                "print(f'\\nDone: {len(essays)} essays, {total_c} total citations')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Preview\n",
                "for e in essays:\n",
                "    print(f\"\\n=== {e['prompt_id']} ({len(e['citations'])} cites) ===\")\n",
                "    print(e['response'][:500])\n",
                "    for c in e['citations'][:3]:\n",
                "        print(f\"  → {c['raw_text']}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Save essays & free generation model\n",
                "import os\n",
                "os.makedirs('results', exist_ok=True)\n",
                "with open('results/essays.json', 'w') as f: json.dump(essays, f, indent=2)\n",
                "print(f'Saved {len(essays)} essays to results/essays.json')\n",
                "\n",
                "del model\n",
                "gc.collect(); torch.cuda.empty_cache()\n",
                "print(f'VRAM after cleanup: {torch.cuda.memory_allocated()/1e9:.2f} GB')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 5. Citation Verification (Semantic Scholar)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import time, requests\n",
                "\n",
                "def verify_citation(authors, year, title=None):\n",
                "    parts = []\n",
                "    if authors: parts.append(authors[0])\n",
                "    if year: parts.append(str(year))\n",
                "    if title: parts.append(title[:50])\n",
                "    if not parts: return 'unverified', 0.0, None\n",
                "    try:\n",
                "        r = requests.get('https://api.semanticscholar.org/graph/v1/paper/search',\n",
                "                         params={'query': ' '.join(parts), 'limit': 5, 'fields': 'title,authors,year'}, timeout=10)\n",
                "        time.sleep(1)\n",
                "        if r.status_code != 200: return 'unverified', 0.0, None\n",
                "        papers = r.json().get('data', [])\n",
                "        if not papers: return 'fabricated', 0.7, None\n",
                "        for p in papers:\n",
                "            yr_ok = (year is None) or (p.get('year') == year)\n",
                "            au_ok = not authors or any(authors[0].split()[-1].lower() in a.get('name','').lower() for a in p.get('authors',[]))\n",
                "            if yr_ok and au_ok: return 'real', 0.85, p.get('title')\n",
                "        return 'fabricated', 0.6, None\n",
                "    except Exception: return 'unverified', 0.0, None\n",
                "\n",
                "stats = {'real': 0, 'fabricated': 0, 'unverified': 0}\n",
                "for essay in tqdm(essays, desc='Verifying'):\n",
                "    for c in essay['citations']:\n",
                "        label, conf, matched = verify_citation(c.get('extracted_authors'), c.get('extracted_year'), c.get('extracted_title'))\n",
                "        c['label'] = label; c['confidence'] = conf; c['matched_title'] = matched\n",
                "        stats[label] += 1\n",
                "\n",
                "total = sum(stats.values())\n",
                "print(f'\\nVerification ({total} citations):')\n",
                "for k, v in stats.items():\n",
                "    print(f'  {k}: {v} ({v/total*100:.0f}%)' if total else f'  {k}: {v}')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 6. Mechanistic Scoring\n",
                "\n",
                "**Strategy for T4 (15GB VRAM):**\n",
                "- **Pass 1:** `output_hidden_states=True, output_attentions=False` → compute ICS, POS, PFS\n",
                "- **Pass 2:** `output_attentions=True, output_hidden_states=False` with `max_length=256` → compute BAS only\n",
                "\n",
                "This avoids the OOM from requesting both simultaneously."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch.nn.functional as F\n",
                "\n",
                "print(f'Loading {MODEL_NAME} for scoring (eager attention)...')\n",
                "scorer_model = AutoModelForCausalLM.from_pretrained(\n",
                "    MODEL_NAME, torch_dtype=torch.float16, device_map='auto', attn_implementation='eager',\n",
                ")\n",
                "scorer_model.eval()\n",
                "NUM_LAYERS = scorer_model.config.num_hidden_layers\n",
                "NUM_HEADS = scorer_model.config.num_attention_heads\n",
                "print(f'Ready: {NUM_LAYERS} layers, {NUM_HEADS} heads, VRAM: {torch.cuda.memory_allocated()/1e9:.1f} GB')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def find_citation_token_positions(essay, tokenizer, max_len):\n",
                "    \"\"\"Map character-level citation positions to token positions.\"\"\"\n",
                "    full_text = essay['prompt'] + essay['response']\n",
                "    enc = tokenizer(full_text, return_tensors='pt', return_offsets_mapping=True,\n",
                "                    truncation=True, max_length=max_len)\n",
                "    offsets = enc.offset_mapping[0].tolist()\n",
                "    prompt_tok_len = len(tokenizer(essay['prompt'], return_tensors='pt').input_ids[0])\n",
                "\n",
                "    results = []\n",
                "    for cite in essay['citations']:\n",
                "        char_start = cite['start_pos'] + len(essay['prompt'])\n",
                "        tok_pos = None\n",
                "        for i, (ts, te) in enumerate(offsets):\n",
                "            if ts <= char_start < te:\n",
                "                tok_pos = i; break\n",
                "        if tok_pos is not None and tok_pos < enc.input_ids.shape[1]:\n",
                "            results.append((cite, tok_pos))\n",
                "    return enc, prompt_tok_len, results\n",
                "\n",
                "print('Utility functions defined.')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "all_scores = []\n",
                "\n",
                "for essay in tqdm(essays, desc='Scoring'):\n",
                "    if not essay['citations']: continue\n",
                "\n",
                "    # --- PASS 1: Hidden states (ICS, POS, PFS) ---\n",
                "    enc, prompt_len, cite_positions = find_citation_token_positions(essay, tokenizer, SCORE_MAX_LEN)\n",
                "    if not cite_positions: continue\n",
                "\n",
                "    try:\n",
                "        with torch.no_grad():\n",
                "            out1 = scorer_model(input_ids=enc.input_ids.cuda(),\n",
                "                                output_hidden_states=True, output_attentions=False, return_dict=True)\n",
                "        hs = [h.cpu() for h in out1.hidden_states]\n",
                "        del out1; torch.cuda.empty_cache()\n",
                "    except Exception as e:\n",
                "        print(f'  HS error {essay[\"prompt_id\"]}: {e}'); continue\n",
                "\n",
                "    # --- PASS 2: Attention (BAS) — shorter max_len to fit ---\n",
                "    enc2, prompt_len2, cite_positions2 = find_citation_token_positions(essay, tokenizer, 256)\n",
                "    bas_per_cite = {}  # tok_pos -> bas_scores\n",
                "    try:\n",
                "        with torch.no_grad():\n",
                "            out2 = scorer_model(input_ids=enc2.input_ids.cuda(),\n",
                "                                output_hidden_states=False, output_attentions=True, return_dict=True)\n",
                "        # attentions is a tuple of (batch, heads, seq, seq) per layer\n",
                "        for cite, tok_pos in cite_positions2:\n",
                "            bas_layers = []\n",
                "            for layer_idx in range(NUM_LAYERS):\n",
                "                # FACTUM BAS: attention FROM citation token TO position 0 (BOS), averaged across heads\n",
                "                # attn shape: [batch, heads, seq, seq]\n",
                "                attn_layer = out2.attentions[layer_idx]  # (1, heads, seq, seq)\n",
                "                bos_attn = attn_layer[0, :, tok_pos, 0].mean().item()  # mean across heads\n",
                "                bas_layers.append(bos_attn)\n",
                "            bas_per_cite[tok_pos] = bas_layers\n",
                "        del out2; torch.cuda.empty_cache()\n",
                "    except Exception as e:\n",
                "        print(f'  BAS error {essay[\"prompt_id\"]}: {e}')\n",
                "\n",
                "    # --- Combine scores per citation ---\n",
                "    for cite, tok_pos in cite_positions:\n",
                "        ics_scores, pos_scores, pfs_scores = [], [], []\n",
                "        for layer in range(NUM_LAYERS):\n",
                "            pre = hs[layer][0, tok_pos]\n",
                "            post = hs[layer + 1][0, tok_pos]\n",
                "            # POS: cosine orthogonality\n",
                "            cos = F.cosine_similarity(pre.unsqueeze(0), post.unsqueeze(0)).item()\n",
                "            pos_scores.append(1.0 - abs(cos))\n",
                "            # PFS: magnitude of hidden state update\n",
                "            pfs_scores.append(torch.norm(post - pre).item() / 2)\n",
                "            # ICS: cosine(citation_hs, mean_essay_hs)\n",
                "            hs_essay = hs[layer + 1][0, prompt_len:tok_pos]\n",
                "            if hs_essay.shape[0] > 0:\n",
                "                ics = F.cosine_similarity(post.unsqueeze(0), hs_essay.mean(0).unsqueeze(0)).item()\n",
                "            else:\n",
                "                ics = 0.0\n",
                "            ics_scores.append(ics)\n",
                "\n",
                "        # BAS from pass 2 (may not exist if tok_pos changed due to shorter truncation)\n",
                "        bas_scores = bas_per_cite.get(tok_pos, [0.0] * NUM_LAYERS)\n",
                "\n",
                "        all_scores.append({\n",
                "            'ics_scores': ics_scores, 'ics_mean': sum(ics_scores)/len(ics_scores), 'ics_final': ics_scores[-1],\n",
                "            'pos_scores': pos_scores, 'pos_mean': sum(pos_scores)/len(pos_scores), 'pos_final': pos_scores[-1],\n",
                "            'pfs_scores': pfs_scores, 'pfs_mean': sum(pfs_scores)/len(pfs_scores), 'pfs_final': pfs_scores[-1],\n",
                "            'bas_scores': bas_scores, 'bas_mean': sum(bas_scores)/len(bas_scores),\n",
                "            'prompt_id': essay['prompt_id'], 'label': cite.get('label', 'unverified'),\n",
                "            'citation': cite['raw_text'], 'tok_pos': tok_pos,\n",
                "            'token': tokenizer.decode(enc.input_ids[0, tok_pos]),\n",
                "        })\n",
                "\n",
                "    del hs; torch.cuda.empty_cache()\n",
                "\n",
                "print(f'\\nScored {len(all_scores)} citations')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 7. Results"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import pandas as pd\n",
                "\n",
                "if all_scores:\n",
                "    df = pd.DataFrame(all_scores)\n",
                "    cols = [c for c in ['prompt_id','citation','label','ics_mean','pos_mean','pfs_mean','bas_mean'] if c in df.columns]\n",
                "    display(df[cols].round(4))\n",
                "    print('\\n=== Averages by Label ===')\n",
                "    num = [c for c in ['ics_mean','pos_mean','pfs_mean','bas_mean'] if c in df.columns]\n",
                "    if len(df['label'].unique()) > 1:\n",
                "        print(df.groupby('label')[num].mean().round(4).to_string())\n",
                "    else:\n",
                "        print(f'All citations have label: {df[\"label\"].iloc[0]}')\n",
                "        print(df[num].describe().round(4).to_string())\n",
                "else:\n",
                "    print('No citations scored.')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import matplotlib.pyplot as plt\n",
                "\n",
                "if all_scores:\n",
                "    fig, axes = plt.subplots(1, 4, figsize=(20, 5))\n",
                "    score_keys = [('ics_scores','ICS'), ('pos_scores','POS'), ('pfs_scores','PFS'), ('bas_scores','BAS')]\n",
                "\n",
                "    for ax, (key, title) in zip(axes, score_keys):\n",
                "        for label in df['label'].unique():\n",
                "            sub = df[df['label'] == label]\n",
                "            means = []\n",
                "            for l in range(NUM_LAYERS):\n",
                "                vals = [s[l] for s in sub[key] if len(s) > l]\n",
                "                means.append(sum(vals)/len(vals) if vals else 0)\n",
                "            color = {'real': 'green', 'fabricated': 'red', 'unverified': 'gray'}.get(label, 'blue')\n",
                "            ax.plot(range(NUM_LAYERS), means, label=label, color=color, alpha=0.8, linewidth=2)\n",
                "        ax.set_title(title, fontsize=14, fontweight='bold')\n",
                "        ax.set_xlabel('Layer'); ax.legend(); ax.grid(True, alpha=0.3)\n",
                "\n",
                "    plt.suptitle(f'Mechanistic Scores — {MODEL_NAME.split(\"/\")[-1]}', fontsize=16, fontweight='bold')\n",
                "    plt.tight_layout(); plt.savefig('results/layer_scores.png', dpi=150, bbox_inches='tight')\n",
                "    plt.show()\n",
                "    print('Saved to results/layer_scores.png')\n",
                "else:\n",
                "    print('No scores to plot.')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Save results\n",
                "scores_out = [{k: ([float(x) for x in v] if isinstance(v, list) else v) for k, v in s.items()} for s in all_scores]\n",
                "with open('results/scores.json', 'w') as f: json.dump(scores_out, f, indent=2)\n",
                "with open('results/essays.json', 'w') as f: json.dump(essays, f, indent=2)\n",
                "print(f'Saved {len(scores_out)} scores and {len(essays)} essays to results/')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 8. Summary Statistics"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "if all_scores:\n",
                "    print(f'Model: {MODEL_NAME}')\n",
                "    print(f'Essays: {len(essays)}')\n",
                "    print(f'Total citations: {sum(len(e[\"citations\"]) for e in essays)}')\n",
                "    print(f'Scored citations: {len(all_scores)}')\n",
                "    print(f'Labels: {dict(df[\"label\"].value_counts())}')\n",
                "    print(f'\\nScore ranges:')\n",
                "    for s in ['ics_mean', 'pos_mean', 'pfs_mean', 'bas_mean']:\n",
                "        if s in df.columns:\n",
                "            print(f'  {s}: {df[s].min():.4f} — {df[s].max():.4f} (mean {df[s].mean():.4f})')"
            ]
        }
    ],
    "metadata": {
        "accelerator": "GPU",
        "colab": {
            "gpuType": "T4",
            "provenance": []
        },
        "kernelspec": {
            "display_name": "Python 3",
            "name": "python3"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 0
}
