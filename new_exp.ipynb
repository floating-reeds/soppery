{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Zero-Shot Citation Hallucination Detection\n",
        "\n",
        "**Scores:** ICS · PAS · PFS · BAS\n",
        "\n",
        "| Score | Full Name | What it measures |\n",
        "|-------|-----------|------------------|\n",
        "| **ICS** | Internal Consistency Score | Cosine sim between citation hidden state and essay context |\n",
        "| **PAS** | Pathway Alignment Score | Cosine sim between attention (read) and FFN (recall) pathways |\n",
        "| **PFS** | Parametric Force Score | Magnitude of the hidden state update (FFN push) |\n",
        "| **BAS** | BOS Attention Score | Attention from citation token to BOS token (position 0) |\n",
        "\n",
        "**Workflow:** Generation and scoring are **fully decoupled**.\n",
        "1. Generate essays one-by-one → each saved to JSONL immediately (crash-safe)\n",
        "2. Push to GitHub → `git pull` on your PC\n",
        "3. (Optional: restart runtime) Load saved essays → Score → Push results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 1. Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GPU: Tesla T4\n"
          ]
        }
      ],
      "source": [
        "!pip install -q transformers accelerate requests tqdm\n",
        "import torch, gc, os, json, time\n",
        "assert torch.cuda.is_available(), 'Switch to GPU runtime!'\n",
        "print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from huggingface_hub import login\n",
        "login(token=\"\")  # <-- paste your HF token"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into '/content/soppery'...\n",
            "remote: Enumerating objects: 5687, done.\u001b[K\n",
            "remote: Counting objects: 100% (5687/5687), done.\u001b[K\n",
            "remote: Compressing objects: 100% (4202/4202), done.\u001b[K\n",
            "remote: Total 5687 (delta 1441), reused 5687 (delta 1441), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (5687/5687), 31.52 MiB | 17.83 MiB/s, done.\n",
            "Resolving deltas: 100% (1441/1441), done.\n",
            "Working in: /content/soppery\n"
          ]
        }
      ],
      "source": [
        "# Clone repo (has prompts, will store results)\n",
        "REPO_DIR = \"/content/soppery\"\n",
        "if not os.path.exists(REPO_DIR):\n",
        "    !git clone https://github.com/floating-reeds/soppery.git {REPO_DIR}\n",
        "else:\n",
        "    !cd {REPO_DIR} && git pull\n",
        "os.chdir(REPO_DIR)\n",
        "!git config user.email \"colab@colab\" && git config user.name \"Colab\"\n",
        "print(f\"Working in: {os.getcwd()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "MODEL_NAME = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
        "MAX_NEW_TOKENS = 1024\n",
        "TEMPERATURE = 0.7\n",
        "SCORE_MAX_LEN = 512"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 2. Load Prompts from Repo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  historical.json: 50 prompts\n",
            "  legal.json: 50 prompts\n",
            "  scientific.json: 50 prompts\n",
            "\n",
            "Total: 150 prompts\n"
          ]
        }
      ],
      "source": [
        "all_prompts = []\n",
        "for fname in sorted(os.listdir('data/prompts')):\n",
        "    if fname.endswith('.json'):\n",
        "        with open(f'data/prompts/{fname}') as f:\n",
        "            prompts = json.load(f)\n",
        "        all_prompts.extend(prompts)\n",
        "        print(f'  {fname}: {len(prompts)} prompts')\n",
        "print(f'\\nTotal: {len(all_prompts)} prompts')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 3. Citation Extraction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test: 1 citations: Vaswani et al. (2017)\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "from dataclasses import dataclass, asdict\n",
        "from typing import List, Optional\n",
        "\n",
        "@dataclass\n",
        "class Citation:\n",
        "    raw_text: str\n",
        "    start_pos: int\n",
        "    end_pos: int\n",
        "    extracted_authors: Optional[List[str]] = None\n",
        "    extracted_year: Optional[int] = None\n",
        "    citation_type: str = \"academic\"\n",
        "\n",
        "PATTERNS = [\n",
        "    re.compile(r'\\(([A-Z][a-z]+(?:\\s+(?:et\\s+al\\.?|&|and)\\s+[A-Z][a-z]+)*,?\\s*\\d{4}[a-z]?)\\)'),\n",
        "    re.compile(r'([A-Z][a-z]+(?:\\s+et\\s+al\\.?))\\s*\\((\\d{4}[a-z]?)\\)'),\n",
        "    re.compile(r'([A-Z][a-z]+(?:\\s+(?:and|&)\\s+[A-Z][a-z]+)?)\\s*\\((\\d{4})\\)'),\n",
        "    re.compile(r'[\"\\u201c]([^\"\\u201d]{10,100})[\"\\u201d]\\s*\\((\\d{4})\\)'),\n",
        "    re.compile(r'([A-Z][a-z]+(?:\\s+[A-Z][a-z]+)*\\s+v\\.?\\s+[A-Z][a-z]+(?:\\s+[A-Z][a-z]+)*)\\s*\\((\\d{4})\\)'),\n",
        "]\n",
        "\n",
        "def extract_citations(text):\n",
        "    cites, seen = [], set()\n",
        "    for pat in PATTERNS:\n",
        "        for m in pat.finditer(text):\n",
        "            s, e = m.span()\n",
        "            if any(s <= p <= e for p in seen): continue\n",
        "            seen.update(range(s, e+1))\n",
        "            c = Citation(raw_text=m.group(0), start_pos=s, end_pos=e)\n",
        "            yr = re.search(r'\\d{4}', m.group(0))\n",
        "            if yr: c.extracted_year = int(yr.group())\n",
        "            au = re.search(r'([A-Z][a-z]+)', m.group(0))\n",
        "            if au: c.extracted_authors = [au.group(1)]\n",
        "            cites.append(c)\n",
        "    return cites\n",
        "\n",
        "# Test\n",
        "t = extract_citations('Vaswani et al. (2017) proposed Transformers. Also see (Devlin et al., 2019).')\n",
        "print(f'Test: {len(t)} citations: {\", \".join(c.raw_text for c in t)}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 4. Generate ALL Essays (Incremental Save)\n",
        "\n",
        "**Crash-safe:** Each essay is appended to a `.jsonl` file immediately after generation.\n",
        "If Colab disconnects, you keep everything generated so far.\n",
        "On restart, it **skips already-generated prompts** automatically."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading meta-llama/Llama-3.2-3B-Instruct...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:104: UserWarning: \n",
            "Error while fetching `HF_TOKEN` secret value from your vault: 'Requesting secret HF_TOKEN timed out. Secrets can only be fetched when running from the Colab UI.'.\n",
            "You are not authenticated with the Hugging Face Hub in this notebook.\n",
            "If the error persists, please let us know by opening an issue on GitHub (https://github.com/huggingface/huggingface_hub/issues/new).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8467edc664604419bad7ad86c33428cc",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/878 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1c8da86a2e384542887999a47a354a79",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/54.5k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0eb3d9f260aa4fbf969c80df727822da",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6261ffc213294962b0aab98c594b4da3",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/296 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`torch_dtype` is deprecated! Use `dtype` instead!\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "cb6d613a6780416e990b0b9574bed8f8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors.index.json:   0%|          | 0.00/20.9k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "649b67931eb74eb0b8ab1eb4cc469a6a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (incomplete total...): 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "21eeb2178adb4c27bc4cee143d452983",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6e2b2448151d455bb78bb6cbc8fc6e25",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading weights:   0%|          | 0/254 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "58a0d35ccff64c35a15872513257bb90",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/189 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded. 28 layers, 24 heads\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from tqdm import tqdm\n",
        "\n",
        "print(f'Loading {MODEL_NAME}...')\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "model = AutoModelForCausalLM.from_pretrained(MODEL_NAME, torch_dtype=torch.float16, device_map='auto')\n",
        "if tokenizer.pad_token is None: tokenizer.pad_token = tokenizer.eos_token\n",
        "print(f'Loaded. {model.config.num_hidden_layers} layers, {model.config.num_attention_heads} heads')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "To generate: 150 essays\n"
          ]
        }
      ],
      "source": [
        "SYSTEM_PROMPTS = {\n",
        "    'scientific': \"\"\"You are an academic researcher writing for a peer-reviewed journal.\n",
        "You MUST include proper academic citations throughout.\n",
        "Format: Author et al. (Year) or (Author et al., Year).\n",
        "Every major claim MUST have a citation with real author names and years.\n",
        "Example: Vaswani et al. (2017) introduced the Transformer architecture.\"\"\",\n",
        "\n",
        "    'legal': \"\"\"You are a legal scholar writing a detailed analysis.\n",
        "You MUST cite relevant court cases: Case Name v. Case Name (Year).\n",
        "Reference specific statutes and legal provisions.\n",
        "Example: In Brown v. Board of Education (1954), the Court ruled...\"\"\",\n",
        "\n",
        "    'historical': \"\"\"You are a historian writing an academic essay.\n",
        "You MUST include academic citations: Author (Year) or (Author, Year).\n",
        "Every claim should reference a historian or primary source.\n",
        "Example: According to Hobsbawm (1962), the Industrial Revolution began...\"\"\",\n",
        "}\n",
        "\n",
        "# Output file — one JSON object per line (append-safe)\n",
        "os.makedirs('data/essays', exist_ok=True)\n",
        "model_short = MODEL_NAME.split('/')[-1]\n",
        "ESSAY_FILE = f'data/essays/{model_short}_essays.jsonl'\n",
        "\n",
        "# Load already-generated IDs (for resume after crash)\n",
        "done_ids = set()\n",
        "if os.path.exists(ESSAY_FILE):\n",
        "    with open(ESSAY_FILE) as f:\n",
        "        for line in f:\n",
        "            try: done_ids.add(json.loads(line)['prompt_id'])\n",
        "            except: pass\n",
        "    print(f'Resuming: {len(done_ids)} essays already generated')\n",
        "\n",
        "remaining = [p for p in all_prompts if p['id'] not in done_ids]\n",
        "print(f'To generate: {len(remaining)} essays')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Generating:   7%|▋         | 10/150 [06:13<1:26:04, 36.89s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  [10/150] 92 new citations\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Generating:  13%|█▎        | 20/150 [12:27<1:21:47, 37.75s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  [20/150] 177 new citations\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Generating:  20%|██        | 30/150 [19:23<1:22:13, 41.11s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  [30/150] 251 new citations\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Generating:  27%|██▋       | 40/150 [26:10<1:12:04, 39.31s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  [40/150] 317 new citations\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Generating:  33%|███▎      | 50/150 [32:33<1:00:28, 36.28s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  [50/150] 389 new citations\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Generating:  40%|████      | 60/150 [38:27<56:30, 37.67s/it]  "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  [60/150] 437 new citations\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Generating:  47%|████▋     | 70/150 [44:44<53:08, 39.86s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  [70/150] 507 new citations\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Generating:  53%|█████▎    | 80/150 [51:26<46:36, 39.95s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  [80/150] 540 new citations\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Generating:  60%|██████    | 90/150 [58:00<40:15, 40.26s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  [90/150] 596 new citations\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Generating:  67%|██████▋   | 100/150 [1:04:01<30:11, 36.23s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  [100/150] 653 new citations\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Generating:  73%|███████▎  | 110/150 [1:10:38<26:04, 39.12s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  [110/150] 676 new citations\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Generating:  80%|████████  | 120/150 [1:17:37<18:52, 37.76s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  [120/150] 724 new citations\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Generating:  87%|████████▋ | 130/150 [1:23:05<10:35, 31.78s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  [130/150] 762 new citations\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Generating:  93%|█████████▎| 140/150 [1:29:58<06:43, 40.34s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  [140/150] 792 new citations\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Generating: 100%|██████████| 150/150 [1:35:58<00:00, 38.39s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  [150/150] 820 new citations\n",
            "\n",
            "Done! Generated 150 essays with 820 citations\n",
            "All saved to: data/essays/Llama-3.2-3B-Instruct_essays.jsonl\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Generate one essay at a time, save immediately\n",
        "total_new_cites = 0\n",
        "\n",
        "for i, p in enumerate(tqdm(remaining, desc='Generating')):\n",
        "    sys_prompt = SYSTEM_PROMPTS.get(p['domain'], SYSTEM_PROMPTS['scientific'])\n",
        "    messages = [{\"role\": \"system\", \"content\": sys_prompt}, {\"role\": \"user\", \"content\": p[\"prompt\"]}]\n",
        "\n",
        "    try:\n",
        "        formatted = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "    except:\n",
        "        formatted = f\"{sys_prompt}\\n\\nRequest: {p['prompt']}\\n\\nResponse:\"\n",
        "\n",
        "    inputs = tokenizer(formatted, return_tensors='pt', truncation=True, max_length=2048).to('cuda')\n",
        "    with torch.no_grad():\n",
        "        out = model.generate(**inputs, max_new_tokens=MAX_NEW_TOKENS, temperature=TEMPERATURE,\n",
        "                             top_p=0.9, do_sample=True, pad_token_id=tokenizer.pad_token_id)\n",
        "\n",
        "    response = tokenizer.decode(out[0][inputs.input_ids.shape[1]:], skip_special_tokens=True).strip()\n",
        "    cites = extract_citations(response)\n",
        "    total_new_cites += len(cites)\n",
        "\n",
        "    essay = {\n",
        "        'prompt_id': p['id'], 'domain': p['domain'], 'prompt': p['prompt'],\n",
        "        'model_name': MODEL_NAME, 'response': response,\n",
        "        'citations': [asdict(c) for c in cites],\n",
        "        'num_citations': len(cites),\n",
        "    }\n",
        "\n",
        "    # Append immediately — crash-safe\n",
        "    with open(ESSAY_FILE, 'a') as f:\n",
        "        f.write(json.dumps(essay) + '\\n')\n",
        "\n",
        "    if (i+1) % 10 == 0:\n",
        "        print(f'  [{i+1}/{len(remaining)}] {total_new_cites} new citations')\n",
        "\n",
        "print(f'\\nDone! Generated {len(remaining)} essays with {total_new_cites} citations')\n",
        "print(f'All saved to: {ESSAY_FILE}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Push essays to GitHub so they appear on your PC\n",
        "!git add data/essays/\n",
        "!git commit -m \"Add generated essays\"\n",
        "!git push\n",
        "print('\\nPushed! Run git pull on your PC to get the essays.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Free GPU for scoring later\n",
        "del model\n",
        "gc.collect(); torch.cuda.empty_cache()\n",
        "print(f'VRAM freed: {torch.cuda.memory_allocated()/1e9:.2f} GB in use')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 5. Citation Verification\n",
        "\n",
        "**What causes `unverified`:**\n",
        "- API timeout / rate-limiting (HTTP 429) after retries\n",
        "- Citation has no extractable author or year (regex found a partial match)\n",
        "- Both Semantic Scholar AND CrossRef are unreachable\n",
        "\n",
        "**How we minimize it:** Retry on failure, CrossRef fallback, broader search queries.\n",
        "\n",
        "If a citation is still `unverified`, it's excluded from scoring (only `real` vs `fabricated` are scored)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import requests\n",
        "\n",
        "def search_ss(query, retries=2):\n",
        "    for _ in range(retries):\n",
        "        try:\n",
        "            r = requests.get('https://api.semanticscholar.org/graph/v1/paper/search',\n",
        "                             params={'query': query, 'limit': 5, 'fields': 'title,authors,year'}, timeout=15)\n",
        "            if r.status_code == 429: time.sleep(3); continue\n",
        "            if r.status_code == 200: return r.json().get('data', [])\n",
        "        except: time.sleep(2)\n",
        "    return None\n",
        "\n",
        "def search_crossref(query):\n",
        "    try:\n",
        "        r = requests.get('https://api.crossref.org/works', params={'query': query, 'rows': 3}, timeout=15)\n",
        "        if r.status_code == 200:\n",
        "            return [{'title': it.get('title',[''])[0],\n",
        "                     'year': it.get('published',{}).get('date-parts',[[None]])[0][0],\n",
        "                     'authors': [{'name': f\"{a.get('given','')} {a.get('family','')}\"} for a in it.get('author',[])]}\n",
        "                    for it in r.json().get('message',{}).get('items',[])]\n",
        "    except: pass\n",
        "    return []\n",
        "\n",
        "def verify_citation(authors, year, raw_text=''):\n",
        "    parts = []\n",
        "    if authors: parts.append(authors[0])\n",
        "    if year: parts.append(str(year))\n",
        "    if not parts:\n",
        "        words = [w for w in raw_text.split() if len(w) > 2][:3]\n",
        "        if words: parts = words\n",
        "    if not parts: return 'unverified', 0.0, None\n",
        "    query = ' '.join(parts)\n",
        "    papers = search_ss(query)\n",
        "    if papers is None:\n",
        "        papers = search_crossref(query)\n",
        "        if not papers: return 'unverified', 0.0, None\n",
        "    if not papers: return 'fabricated', 0.75, None\n",
        "    for p in papers:\n",
        "        yr_ok = (year is None) or (p.get('year') == year)\n",
        "        au_ok = True\n",
        "        if authors:\n",
        "            last = authors[0].split()[-1].lower()\n",
        "            au_ok = any(last in a.get('name','').lower() for a in p.get('authors',[]))\n",
        "        if yr_ok and au_ok: return 'real', 0.9, p.get('title')\n",
        "    return 'fabricated', 0.6, None\n",
        "\n",
        "print('Verification functions defined (SS + CrossRef fallback)')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load all generated essays\n",
        "essays = []\n",
        "with open(ESSAY_FILE) as f:\n",
        "    for line in f:\n",
        "        try: essays.append(json.loads(line))\n",
        "        except: pass\n",
        "print(f'Loaded {len(essays)} essays')\n",
        "\n",
        "# Verify\n",
        "stats = {'real': 0, 'fabricated': 0, 'unverified': 0}\n",
        "for essay in tqdm(essays, desc='Verifying'):\n",
        "    for c in essay['citations']:\n",
        "        if 'label' in c: stats[c['label']] += 1; continue  # Skip already verified\n",
        "        label, conf, matched = verify_citation(c.get('extracted_authors'), c.get('extracted_year'), c.get('raw_text',''))\n",
        "        c['label'] = label; c['confidence'] = conf; c['matched_title'] = matched\n",
        "        stats[label] += 1\n",
        "        time.sleep(0.5)\n",
        "\n",
        "total = sum(stats.values())\n",
        "print(f'\\nVerification ({total} citations):')\n",
        "for k, v in stats.items(): print(f'  {k}: {v} ({v/total*100:.0f}%)' if total else f'  {k}: {v}')\n",
        "\n",
        "# Overwrite JSONL with verified versions\n",
        "with open(ESSAY_FILE, 'w') as f:\n",
        "    for e in essays: f.write(json.dumps(e) + '\\n')\n",
        "print(f'Saved verified essays')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 6. Mechanistic Scoring (ICS, PAS, PFS, BAS)\n",
        "\n",
        "**You can run this anytime** — essays are loaded from the saved JSONL file.\n",
        "Generation and scoring are completely independent.\n",
        "\n",
        "⚠️ If you just ran generation, **restart the runtime** first to free VRAM,\n",
        "then re-run cells 1–3 (setup/login/clone) and skip to here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load essays (works after runtime restart)\n",
        "import json, os, gc, torch, time\n",
        "import torch.nn.functional as F\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from tqdm import tqdm\n",
        "\n",
        "MODEL_NAME = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
        "SCORE_MAX_LEN = 512\n",
        "os.chdir('/content/soppery')\n",
        "ESSAY_FILE = f'data/essays/{MODEL_NAME.split(\"/\")[-1]}_essays.jsonl'\n",
        "\n",
        "essays = []\n",
        "with open(ESSAY_FILE) as f:\n",
        "    for line in f:\n",
        "        try: essays.append(json.loads(line))\n",
        "        except: pass\n",
        "\n",
        "scorable = [e for e in essays if any(c.get('label') in ('real','fabricated') for c in e.get('citations',[]))]\n",
        "print(f'Loaded {len(essays)} essays, {len(scorable)} have real/fabricated citations')\n",
        "\n",
        "print(f'Loading {MODEL_NAME} with eager attention...')\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "if tokenizer.pad_token is None: tokenizer.pad_token = tokenizer.eos_token\n",
        "scorer = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME, torch_dtype=torch.float16, device_map='auto', attn_implementation='eager')\n",
        "scorer.eval()\n",
        "NL = scorer.config.num_hidden_layers\n",
        "print(f'Ready: {NL} layers, VRAM: {torch.cuda.memory_allocated()/1e9:.1f} GB')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- HOOKS: separate attention vs FFN pathways ---\n",
        "activation_cache = {}\n",
        "def get_activation(name):\n",
        "    def hook(model, input, output):\n",
        "        activation_cache[name] = output[0].detach()\n",
        "    return hook\n",
        "\n",
        "for i in range(NL):\n",
        "    scorer.model.layers[i].post_attention_layernorm.register_forward_hook(get_activation(f'layer_{i}_mid'))\n",
        "\n",
        "def find_cite_toks(essay, tok, ml):\n",
        "    txt = essay['prompt'] + essay['response']\n",
        "    enc = tok(txt, return_tensors='pt', return_offsets_mapping=True, truncation=True, max_length=ml)\n",
        "    offs = enc.offset_mapping[0].tolist()\n",
        "    plen = len(tok(essay['prompt'], return_tensors='pt').input_ids[0])\n",
        "    res = []\n",
        "    for c in essay['citations']:\n",
        "        if c.get('label') not in ('real','fabricated'): continue\n",
        "        cs = c['start_pos'] + len(essay['prompt'])\n",
        "        for i,(ts,te) in enumerate(offs):\n",
        "            if ts <= cs < te: res.append((c,i)); break\n",
        "    return enc, plen, res\n",
        "\n",
        "all_scores = []\n",
        "for essay in tqdm(scorable, desc='Scoring'):\n",
        "    enc, plen, cps = find_cite_toks(essay, tokenizer, SCORE_MAX_LEN)\n",
        "    if not cps: continue\n",
        "\n",
        "    try:\n",
        "        activation_cache = {}\n",
        "        with torch.no_grad():\n",
        "            out = scorer(input_ids=enc.input_ids.cuda(),\n",
        "                         output_hidden_states=True, output_attentions=True, return_dict=True)\n",
        "        hs = tuple(h.cpu() for h in out.hidden_states)\n",
        "        attn = torch.stack(out.attentions).squeeze(1).cpu()\n",
        "        del out; torch.cuda.empty_cache()\n",
        "    except Exception as e:\n",
        "        print(f'  Error {essay[\"prompt_id\"]}: {e}'); continue\n",
        "\n",
        "    for c, tp in cps:\n",
        "        ics, pas, pfs, bas = [], [], [], []\n",
        "        for l in range(NL):\n",
        "            pre  = hs[l][0, tp].cuda()\n",
        "            mid  = activation_cache[f'layer_{l}_mid'][0, tp].cuda()\n",
        "            post = hs[l+1][0, tp].cuda()\n",
        "\n",
        "            v_attn = mid - pre\n",
        "            v_ffn  = post - mid\n",
        "\n",
        "            # ICS: attention-weighted context similarity\n",
        "            if l < attn.shape[0]:\n",
        "                a = attn[l, :, tp, plen:tp].cuda()\n",
        "                if a.shape[-1] > 0:\n",
        "                    a = a / (a.sum(dim=-1, keepdim=True) + 1e-9)\n",
        "                    a = a.to(dtype=torch.float16)\n",
        "                    ctx = torch.matmul(a, hs[l+1][0, plen:tp].cuda())\n",
        "                    ics_val = F.cosine_similarity(ctx, post.unsqueeze(0).expand_as(ctx), dim=-1).mean().item()\n",
        "                else: ics_val = 0.0\n",
        "            else: ics_val = 0.0\n",
        "            ics.append(ics_val)\n",
        "\n",
        "            # PAS: cosine sim between attn and FFN pathways\n",
        "            if torch.norm(v_attn) > 1e-6 and torch.norm(v_ffn) > 1e-6:\n",
        "                pas.append(F.cosine_similarity(v_attn.unsqueeze(0), v_ffn.unsqueeze(0)).item())\n",
        "            else:\n",
        "                pas.append(0.0)\n",
        "\n",
        "            # PFS: FFN force magnitude\n",
        "            pfs.append(torch.norm(v_ffn).item())\n",
        "\n",
        "            # BAS: attention to BOS\n",
        "            if l < attn.shape[0]:\n",
        "                bas.append(attn[l, :, tp, 0].mean().item())\n",
        "            else: bas.append(0.0)\n",
        "\n",
        "        all_scores.append({\n",
        "            'ics_scores': ics, 'ics_mean': sum(ics)/len(ics), 'ics_final': ics[-1],\n",
        "            'pas_scores': pas, 'pas_mean': sum(pas)/len(pas), 'pas_final': pas[-1],\n",
        "            'pfs_scores': pfs, 'pfs_mean': sum(pfs)/len(pfs), 'pfs_final': pfs[-1],\n",
        "            'bas_scores': bas, 'bas_mean': sum(bas)/len(bas),\n",
        "            'prompt_id': essay['prompt_id'], 'domain': essay['domain'],\n",
        "            'label': c.get('label','?'), 'citation': c['raw_text'],\n",
        "        })\n",
        "    del hs, attn; torch.cuda.empty_cache()\n",
        "\n",
        "print(f'\\nScored {len(all_scores)} citations')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 7. Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "if all_scores:\n",
        "    df = pd.DataFrame(all_scores)\n",
        "    cols = [c for c in ['prompt_id','domain','citation','label','ics_mean','pas_mean','pfs_mean','bas_mean'] if c in df.columns]\n",
        "    display(df[cols].round(4))\n",
        "    print('\\n=== Averages by Label ===')\n",
        "    num = [c for c in ['ics_mean','pas_mean','pfs_mean','bas_mean'] if c in df.columns]\n",
        "    if len(df['label'].unique()) > 1:\n",
        "        print(df.groupby('label')[num].mean().round(4).to_string())\n",
        "    else:\n",
        "        print(f'All: {df[\"label\"].iloc[0]}')\n",
        "        print(df[num].describe().round(4).to_string())\n",
        "else:\n",
        "    print('No citations scored.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "if all_scores and len(df['label'].unique()) > 1:\n",
        "    fig, axes = plt.subplots(1, 4, figsize=(20, 5))\n",
        "    for ax, (key, title) in zip(axes, [('ics_scores','ICS'),('pas_scores','PAS'),('pfs_scores','PFS'),('bas_scores','BAS')]):\n",
        "        for label in sorted(df['label'].unique()):\n",
        "            sub = df[df['label']==label]\n",
        "            means = []\n",
        "            for l in range(NL):\n",
        "                vals = [s[l] for s in sub[key] if len(s)>l]\n",
        "                means.append(sum(vals)/len(vals) if vals else 0)\n",
        "            color = {'real':'green','fabricated':'red'}.get(label,'gray')\n",
        "            style = '--' if label == 'fabricated' else '-'\n",
        "            ax.plot(range(NL), means, label=label, color=color, alpha=0.8, lw=2, linestyle=style)\n",
        "        ax.set_title(title, fontsize=14, fontweight='bold')\n",
        "        ax.set_xlabel('Layer'); ax.legend(); ax.grid(True, alpha=0.3)\n",
        "    plt.suptitle(f'Mechanistic Signatures — {MODEL_NAME.split(\"/\")[-1]}', fontsize=16, fontweight='bold')\n",
        "    plt.tight_layout()\n",
        "    os.makedirs('data/results', exist_ok=True)\n",
        "    plt.savefig('data/results/layer_scores.png', dpi=150, bbox_inches='tight')\n",
        "    plt.show()\n",
        "else:\n",
        "    print('Need both real and fabricated citations for comparison plots')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save scores & push to GitHub\n",
        "os.makedirs('data/results', exist_ok=True)\n",
        "sc_out = [{k: ([float(x) for x in v] if isinstance(v, list) else v) for k, v in s.items()} for s in all_scores]\n",
        "scores_file = f'data/results/{MODEL_NAME.split(\"/\")[-1]}_scores.json'\n",
        "with open(scores_file, 'w') as f: json.dump(sc_out, f, indent=2)\n",
        "with open(ESSAY_FILE, 'w') as f:\n",
        "    for e in essays: f.write(json.dumps(e) + '\\n')\n",
        "\n",
        "!git add data/\n",
        "!git commit -m \"Add scores and verified essays\"\n",
        "!git push\n",
        "print(f'\\nPushed! git pull on your PC to get results.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 8. Interpreting the Graphs\n",
        "\n",
        "Here's what to look for in each plot:\n",
        "\n",
        "### ICS (Internal Consistency Score)\n",
        "- Measures how much the citation \"fits\" with the surrounding essay text\n",
        "- **Expected:** Real citations should have **higher ICS** (more consistent with context) in later layers where semantic meaning is encoded\n",
        "- **If real ≈ fabricated:** The model generates both types with similar internal consistency — it doesn't \"know\" the citation is fake at the representation level\n",
        "\n",
        "### PAS (Pathway Alignment Score)\n",
        "- Measures internal agreement between attention (what the model reads) and FFN (what it recalls from parameters)\n",
        "- **Expected:** Fabricated citations should show **higher PAS** (more internal conflict) in middle layers where factual knowledge is stored\n",
        "- **If both spike in early layers:** The early layers are doing general token processing — the interesting signal is in **layers 10–20** for a 28-layer model\n",
        "\n",
        "### PFS (Parametric Force Score)  \n",
        "- Raw magnitude of the hidden state update — how hard the FFN is \"pushing\"\n",
        "- **Expected:** Fabricated citations need stronger parametric force since they aren't grounded in real knowledge\n",
        "- **If similar:** The model applies similar computational effort to both types\n",
        "\n",
        "### BAS (BOS Attention Score)\n",
        "- How much the citation token attends to the BOS (beginning-of-sequence) token\n",
        "- **Expected:** Higher BAS = model is relying more on parametric memory. Fabricated citations may show **different BAS patterns** in middle/late layers\n",
        "- **Bell curve pattern (peaking layers 5-10):** Normal — BOS acts as a \"memory sink\" in transformer models\n",
        "\n",
        "### What if curves overlap heavily?\n",
        "This means the 3B model's internal representations don't clearly distinguish real from fabricated citations. Possible reasons:\n",
        "1. The model is too small to have separable mechanistic signatures\n",
        "2. More data (more essays) is needed for the averages to separate\n",
        "3. The zero-shot setting makes it harder (no external context to contrast against)\n",
        "4. Different layer ranges or per-head analysis might reveal subtler signals"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
