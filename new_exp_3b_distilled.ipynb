{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zaoedViGV4ya"
      },
      "source": [
        "# Zero-Shot Citation Hallucination Detection\n",
        "\n",
        "**Scores:** ICS · PAS · PFS · BAS\n",
        "\n",
        "| Score | Full Name | What it measures |\n",
        "|-------|-----------|------------------|\n",
        "| **ICS** | Internal Consistency Score | Cosine sim between citation hidden state and essay context |\n",
        "| **PAS** | Pathway Alignment Score | Cosine sim between attention (read) and FFN (recall) pathways |\n",
        "| **PFS** | Parametric Force Score | Magnitude of the hidden state update (FFN push) |\n",
        "| **BAS** | BOS Attention Score | Attention from citation token to BOS token (position 0) |\n",
        "\n",
        "**Workflow:** Generation and scoring are **fully decoupled**.\n",
        "1. Generate essays one-by-one → each saved to JSONL immediately (crash-safe)\n",
        "2. Push to GitHub → `git pull` on your PC\n",
        "3. (Optional: restart runtime) Load saved essays → Score → Push results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qNgyNM14V4ys"
      },
      "source": [
        "---\n",
        "## 1. Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2lBXGewEV4yw",
        "outputId": "44864944-af2a-4f2c-d539-c13000af386a"
      },
      "outputs": [],
      "source": [
        "!pip install -q transformers accelerate requests tqdm\n",
        "import torch, gc, os, json, time\n",
        "assert torch.cuda.is_available(), 'Switch to GPU runtime!'\n",
        "print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tq3XNnSeV4y-"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import login\n",
        "login(token=\"\")  # <-- paste your HF token"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kamQKgOeV4zA",
        "outputId": "2fc0dd0c-dcf8-4019-a269-17ded530d4ee"
      },
      "outputs": [],
      "source": [
        "# Clone repo (has prompts, will store results)\n",
        "REPO_DIR = \"/content/soppery\"\n",
        "if not os.path.exists(REPO_DIR):\n",
        "    !git clone https://github.com/floating-reeds/soppery.git {REPO_DIR}\n",
        "else:\n",
        "    !cd {REPO_DIR} && git pull\n",
        "os.chdir(REPO_DIR)\n",
        "\n",
        "# Mount Google Drive for auto-saving results\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "DRIVE_BACKUP = '/content/drive/MyDrive/soppery_results'\n",
        "os.makedirs(DRIVE_BACKUP, exist_ok=True)\n",
        "print(f\"Working in: {os.getcwd()}\")\n",
        "print(f\"Drive backup: {DRIVE_BACKUP}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5nIPf17tV4zA"
      },
      "outputs": [],
      "source": [
        "# Llama 3.2 3B Instruct (knowledge-distilled from Llama 3.1)\n",
        "# A smaller distilled model from the Llama 3.2 family.\nRequires HuggingFace access approval: https://huggingface.co/meta-llama/Llama-3.2-3B-Instruct\n",
        "\n",
        "from datetime import datetime\n",
        "RUN_ID = datetime.now().strftime(\"%Y%m%d_%H%M\")\n",
        "\n",
        "MODEL_NAME = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
        "MAX_NEW_TOKENS = 512\n",
        "TEMPERATURE = 0.7\n",
        "SCORE_MAX_LEN = 512\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5JHSJaImV4zB"
      },
      "source": [
        "---\n",
        "## 2. Load Prompts from Repo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Zm79Z3SV4zE",
        "outputId": "4d9bf8fc-8230-4c14-a70b-63052d129a28"
      },
      "outputs": [],
      "source": [
        "all_prompts = []\n",
        "for fname in sorted(os.listdir('data/prompts')):\n",
        "    if fname.endswith('.json'):\n",
        "        with open(f'data/prompts/{fname}') as f:\n",
        "            prompts = json.load(f)\n",
        "        all_prompts.extend(prompts)\n",
        "        print(f'  {fname}: {len(prompts)} prompts')\n",
        "print(f'\\nTotal: {len(all_prompts)} prompts')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AFIJgf7GV4zK"
      },
      "source": [
        "---\n",
        "## 3. Citation Extraction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tojo-5d9V4zO",
        "outputId": "7a2bc79a-3d24-4c99-8af8-8af96fc47113"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "from dataclasses import dataclass, asdict\n",
        "from typing import List, Optional\n",
        "\n",
        "@dataclass\n",
        "class Citation:\n",
        "    raw_text: str\n",
        "    start_pos: int\n",
        "    end_pos: int\n",
        "    extracted_authors: Optional[List[str]] = None\n",
        "    extracted_year: Optional[int] = None\n",
        "    citation_type: str = \"academic\"\n",
        "\n",
        "PATTERNS = [\n",
        "    re.compile(r'\\(([A-Z][a-z]+(?:\\s+(?:et\\s+al\\.?|&|and)\\s+[A-Z][a-z]+)*,?\\s*\\d{4}[a-z]?)\\)'),\n",
        "    re.compile(r'([A-Z][a-z]+(?:\\s+et\\s+al\\.?))\\s*\\((\\d{4}[a-z]?)\\)'),\n",
        "    re.compile(r'([A-Z][a-z]+(?:\\s+(?:and|&)\\s+[A-Z][a-z]+)?)\\s*\\((\\d{4})\\)'),\n",
        "    re.compile(r'[\"\\u201c]([^\"\\u201d]{10,100})[\"\\u201d]\\s*\\((\\d{4})\\)'),\n",
        "    re.compile(r'([A-Z][a-z]+(?:\\s+[A-Z][a-z]+)*\\s+v\\.?\\s+[A-Z][a-z]+(?:\\s+[A-Z][a-z]+)*)\\s*\\((\\d{4})\\)'),\n",
        "]\n",
        "\n",
        "def extract_citations(text):\n",
        "    cites, seen = [], set()\n",
        "    for pat in PATTERNS:\n",
        "        for m in pat.finditer(text):\n",
        "            s, e = m.span()\n",
        "            if any(s <= p <= e for p in seen): continue\n",
        "            seen.update(range(s, e+1))\n",
        "            c = Citation(raw_text=m.group(0), start_pos=s, end_pos=e)\n",
        "            yr = re.search(r'\\d{4}', m.group(0))\n",
        "            if yr: c.extracted_year = int(yr.group())\n",
        "            au = re.search(r'([A-Z][a-z]+)', m.group(0))\n",
        "            if au: c.extracted_authors = [au.group(1)]\n",
        "            cites.append(c)\n",
        "    return cites\n",
        "\n",
        "# Test\n",
        "t = extract_citations('Vaswani et al. (2017) proposed Transformers. Also see (Devlin et al., 2019).')\n",
        "print(f'Test: {len(t)} citations: {\", \".join(c.raw_text for c in t)}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w9VVIaioV4zP"
      },
      "source": [
        "---\n",
        "## 4. Generate ALL Essays (Incremental Save)\n",
        "\n",
        "**Crash-safe:** Each essay is appended to a `.jsonl` file immediately after generation.\n",
        "If Colab disconnects, you keep everything generated so far.\n",
        "On restart, it **skips already-generated prompts** automatically."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 112,
          "referenced_widgets": [
            "b09c5a9530524af681d076f478f77670",
            "77af65392a2145cab3a5296c1011d22e",
            "2e423849c8a04940a0d296434ff29df6",
            "cdcde1814359405994676994bf6f7126",
            "6c17268083044faa9be007fcdbf81f8f",
            "7f839efd9a87457e9253e10c294b3237",
            "e35313ada2964a5f93596fd7c5188892",
            "e2eeca48b1c945c68fe3ad0929dc5c13",
            "8deba9c86af44d8ab5a3fa36b5ae4220",
            "2bea0ee09c554802ba79f72671af0b67",
            "b6827ba888f1484d888c091bb3b0c71e"
          ]
        },
        "id": "kaOoLXAJV4zQ",
        "outputId": "21f5be8b-3ac5-4746-bb93-ac306db1f63a"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from tqdm import tqdm\n",
        "\n",
        "print(f'Loading {MODEL_NAME}...')\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "model = AutoModelForCausalLM.from_pretrained(MODEL_NAME, torch_dtype=torch.float16, device_map='auto')\n",
        "if tokenizer.pad_token is None: tokenizer.pad_token = tokenizer.eos_token\n",
        "print(f'Loaded. {model.config.num_hidden_layers} layers, {model.config.num_attention_heads} heads')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZUrpNznpV4zS",
        "outputId": "3b909fb0-6baa-4fa6-9dd3-3b329842e62f"
      },
      "outputs": [],
      "source": [
        "SYSTEM_PROMPTS = {\n",
        "    'scientific': \"\"\"You are an academic researcher writing a concise research summary.\n",
        "Write a brief, focused response (around 300-400 words).\n",
        "You MUST include proper academic citations throughout.\n",
        "Format: Author et al. (Year) or (Author et al., Year).\n",
        "Every major claim MUST have a citation with real author names and years.\n",
        "Example: Vaswani et al. (2017) introduced the Transformer architecture.\"\"\",\n",
        "}\n",
        "\n",
        "# Output file — one JSON object per line (append-safe)\n",
        "os.makedirs('data/essays', exist_ok=True)\n",
        "model_short = MODEL_NAME.split('/')[-1]\n",
        "ESSAY_FILE = f'data/essays/{model_short}_{RUN_ID}_essays.jsonl'\n",
        "\n",
        "# Load already-generated IDs (for resume after crash)\n",
        "done_ids = set()\n",
        "if os.path.exists(ESSAY_FILE):\n",
        "    with open(ESSAY_FILE) as f:\n",
        "        for line in f:\n",
        "            try: done_ids.add(json.loads(line)['prompt_id'])\n",
        "            except: pass\n",
        "    print(f'Resuming: {len(done_ids)} essays already generated')\n",
        "\n",
        "remaining = [p for p in all_prompts if p['id'] not in done_ids]\n",
        "print(f'To generate: {len(remaining)} essays')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x2QCQK3WV4zU",
        "outputId": "2cb43c43-c1b9-4ad1-9677-8660bb28e98c"
      },
      "outputs": [],
      "source": [
        "import shutil\n",
        "\n",
        "def save_to_drive():\n",
        "    \"\"\"Copy essay file to Google Drive after each generation.\"\"\"\n",
        "    try:\n",
        "        shutil.copy2(ESSAY_FILE, f'{DRIVE_BACKUP}/{os.path.basename(ESSAY_FILE)}')\n",
        "    except Exception:\n",
        "        pass  # Don't let Drive errors stop generation\n",
        "\n",
        "# Generate one essay at a time, save immediately\n",
        "total_new_cites = 0\n",
        "\n",
        "for i, p in enumerate(tqdm(remaining, desc='Generating')):\n",
        "    sys_prompt = SYSTEM_PROMPTS.get(p['domain'], SYSTEM_PROMPTS['scientific'])\n",
        "    messages = [{\"role\": \"system\", \"content\": sys_prompt}, {\"role\": \"user\", \"content\": p[\"prompt\"]}]\n",
        "\n",
        "    try:\n",
        "        formatted = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "    except:\n",
        "        formatted = f\"{sys_prompt}\\n\\nRequest: {p['prompt']}\\n\\nResponse:\"\n",
        "\n",
        "    inputs = tokenizer(formatted, return_tensors='pt', truncation=True, max_length=2048).to('cuda')\n",
        "    with torch.no_grad():\n",
        "        out = model.generate(**inputs, max_new_tokens=MAX_NEW_TOKENS, temperature=TEMPERATURE,\n",
        "                             top_p=0.9, do_sample=True, pad_token_id=tokenizer.pad_token_id)\n",
        "\n",
        "    response = tokenizer.decode(out[0][inputs.input_ids.shape[1]:], skip_special_tokens=True).strip()\n",
        "    cites = extract_citations(response)\n",
        "    total_new_cites += len(cites)\n",
        "\n",
        "    essay = {\n",
        "        'prompt_id': p['id'], 'domain': p['domain'], 'prompt': p['prompt'],\n",
        "        'model_name': MODEL_NAME, 'response': response,\n",
        "        'citations': [asdict(c) for c in cites],\n",
        "        'num_citations': len(cites),\n",
        "    }\n",
        "\n",
        "    # Append immediately — crash-safe\n",
        "    with open(ESSAY_FILE, 'a') as f:\n",
        "        f.write(json.dumps(essay) + '\\n')\n",
        "\n",
        "    # Backup to Google Drive after each essay\n",
        "    save_to_drive()\n",
        "\n",
        "    if (i+1) % 10 == 0:\n",
        "        print(f'  [{i+1}/{len(remaining)}] {total_new_cites} new citations')\n",
        "\n",
        "print(f'\\nDone! Generated {len(remaining)} essays with {total_new_cites} citations')\n",
        "print(f'All saved to: {ESSAY_FILE}')\n",
        "print(f'Drive backup: {DRIVE_BACKUP}/{os.path.basename(ESSAY_FILE)}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2u39eCAXV4zg",
        "outputId": "b1bf7501-a680-40c1-cc34-352799b9b729"
      },
      "outputs": [],
      "source": [
        "# Copy essays to Google Drive\n",
        "import shutil\n",
        "shutil.copy2(ESSAY_FILE, f'{DRIVE_BACKUP}/{os.path.basename(ESSAY_FILE)}')\n",
        "print(f'Essays backed up to Drive: {DRIVE_BACKUP}/')\n",
        "print('Check Google Drive > MyDrive > soppery_results')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z7EBSjDiV4zj",
        "outputId": "7c855633-efe1-4c6f-c4fb-73a3e5415af7"
      },
      "outputs": [],
      "source": [
        "# Free GPU for scoring later\n",
        "del model\n",
        "gc.collect(); torch.cuda.empty_cache()\n",
        "print(f'VRAM freed: {torch.cuda.memory_allocated()/1e9:.2f} GB in use')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vqcYq8prV400"
      },
      "source": [
        "---\n",
        "## 5. Citation Verification\n",
        "\n",
        "**What causes `unverified`:**\n",
        "- API timeout / rate-limiting (HTTP 429) after retries\n",
        "- Citation has no extractable author or year (regex found a partial match)\n",
        "- Both Semantic Scholar AND CrossRef are unreachable\n",
        "\n",
        "**How we minimize it:** Retry on failure, CrossRef fallback, broader search queries.\n",
        "\n",
        "If a citation is still `unverified`, it's excluded from scoring (only `real` vs `fabricated` are scored)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xhJWSuBCV401",
        "outputId": "216d321d-0b3b-4776-d433-476f9f56d347"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "\n",
        "def search_ss(query, retries=2):\n",
        "    for _ in range(retries):\n",
        "        try:\n",
        "            r = requests.get('https://api.semanticscholar.org/graph/v1/paper/search',\n",
        "                             params={'query': query, 'limit': 5, 'fields': 'title,authors,year'}, timeout=15)\n",
        "            if r.status_code == 429: time.sleep(3); continue\n",
        "            if r.status_code == 200: return r.json().get('data', [])\n",
        "        except: time.sleep(2)\n",
        "    return None\n",
        "\n",
        "def search_crossref(query):\n",
        "    try:\n",
        "        r = requests.get('https://api.crossref.org/works', params={'query': query, 'rows': 3}, timeout=15)\n",
        "        if r.status_code == 200:\n",
        "            return [{'title': it.get('title',[''])[0],\n",
        "                     'year': it.get('published',{}).get('date-parts',[[None]])[0][0],\n",
        "                     'authors': [{'name': f\"{a.get('given','')} {a.get('family','')}\"} for a in it.get('author',[])]}\n",
        "                    for it in r.json().get('message',{}).get('items',[])]\n",
        "    except: pass\n",
        "    return []\n",
        "\n",
        "def verify_citation(authors, year, raw_text=''):\n",
        "    parts = []\n",
        "    if authors: parts.append(authors[0])\n",
        "    if year: parts.append(str(year))\n",
        "    if not parts:\n",
        "        words = [w for w in raw_text.split() if len(w) > 2][:3]\n",
        "        if words: parts = words\n",
        "    if not parts: return 'unverified', 0.0, None\n",
        "    query = ' '.join(parts)\n",
        "    papers = search_ss(query)\n",
        "    if papers is None:\n",
        "        papers = search_crossref(query)\n",
        "        if not papers: return 'unverified', 0.0, None\n",
        "    if not papers: return 'fabricated', 0.75, None\n",
        "    for p in papers:\n",
        "        yr_ok = (year is None) or (p.get('year') == year)\n",
        "        au_ok = True\n",
        "        if authors:\n",
        "            last = authors[0].split()[-1].lower()\n",
        "            au_ok = any(last in a.get('name','').lower() for a in p.get('authors',[]))\n",
        "        if yr_ok and au_ok: return 'real', 0.9, p.get('title')\n",
        "    return 'fabricated', 0.6, None\n",
        "\n",
        "print('Verification functions defined (SS + CrossRef fallback)')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VvGhXIK0V41G",
        "outputId": "a04f69ce-7f7c-45cc-dd73-c28c46e65b54"
      },
      "outputs": [],
      "source": [
        "# Load all generated essays\n",
        "essays = []\n",
        "with open(ESSAY_FILE) as f:\n",
        "    for line in f:\n",
        "        try: essays.append(json.loads(line))\n",
        "        except: pass\n",
        "print(f'Loaded {len(essays)} essays')\n",
        "\n",
        "# Verify\n",
        "stats = {'real': 0, 'fabricated': 0, 'unverified': 0}\n",
        "for essay in tqdm(essays, desc='Verifying'):\n",
        "    for c in essay['citations']:\n",
        "        if 'label' in c: stats[c['label']] += 1; continue  # Skip already verified\n",
        "        label, conf, matched = verify_citation(c.get('extracted_authors'), c.get('extracted_year'), c.get('raw_text',''))\n",
        "        c['label'] = label; c['confidence'] = conf; c['matched_title'] = matched\n",
        "        stats[label] += 1\n",
        "        time.sleep(0.5)\n",
        "\n",
        "total = sum(stats.values())\n",
        "print(f'\\nVerification ({total} citations):')\n",
        "for k, v in stats.items(): print(f'  {k}: {v} ({v/total*100:.0f}%)' if total else f'  {k}: {v}')\n",
        "\n",
        "# Overwrite JSONL with verified versions\n",
        "with open(ESSAY_FILE, 'w') as f:\n",
        "    for e in essays: f.write(json.dumps(e) + '\\n')\n",
        "print(f'Saved verified essays')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GCBo-hOIV41j"
      },
      "source": [
        "---\n",
        "## 6. Mechanistic Scoring (ICS, PAS, PFS, BAS)\n",
        "\n",
        "**You can run this anytime** — essays are loaded from the saved JSONL file.\n",
        "Generation and scoring are completely independent.\n",
        "\n",
        "⚠️ If you just ran generation, **restart the runtime** first to free VRAM,\n",
        "then re-run cells 1–3 (setup/login/clone) and skip to here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 129,
          "referenced_widgets": [
            "05a11b9ed95049a3bae105625198156a",
            "18a1659ce2d44fe7bce72d3034265b0f",
            "7d6fdf10e1ef48fb9c2547f2b45e8a8b",
            "caf1204314ca4fa794481bcb75cc8419",
            "60e0d2f1406e4ef194e03fad9728d00a",
            "11b45aa892cd41eab33b978a1072c2ca",
            "2804673dd6eb4128a45d763eb3b588b4",
            "01cfd4b58b424150a16f9c03141ccfb5",
            "6f6e67ba1a8e4dfba66c071b90176b82",
            "63034f07791a466b8ced2e5de8425c93",
            "9388dd157ed7414aad44ae804c67fa36"
          ]
        },
        "id": "sr5XRugeV41l",
        "outputId": "ccea1876-effe-49c9-9443-84c14d93525a"
      },
      "outputs": [],
      "source": [
        "# Load essays (works after runtime restart)\n",
        "import json, os, gc, torch, time\n",
        "import torch.nn.functional as F\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from tqdm import tqdm\n",
        "\n",
        "from datetime import datetime\n",
        "\n",
        "MODEL_NAME = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
        "SCORE_MAX_LEN = 512\n",
        "os.chdir('/content/soppery')\n",
        "\n",
        "# Find the most recent essay file for this model\n",
        "model_short = MODEL_NAME.split('/')[-1]\n",
        "import glob\n",
        "essay_files = sorted(glob.glob(f'data/essays/{model_short}_*_essays.jsonl'))\n",
        "if essay_files:\n",
        "    ESSAY_FILE = essay_files[-1]  # most recent by filename\n",
        "else:\n",
        "    ESSAY_FILE = f'data/essays/{model_short}_essays.jsonl'  # fallback\n",
        "\n",
        "essays = []\n",
        "with open(ESSAY_FILE) as f:\n",
        "    for line in f:\n",
        "        try: essays.append(json.loads(line))\n",
        "        except: pass\n",
        "\n",
        "scorable = [e for e in essays if any(c.get('label') in ('real','fabricated') for c in e.get('citations',[]))]\n",
        "print(f'Loaded {len(essays)} essays from {ESSAY_FILE}')\n",
        "print(f'{len(scorable)} have real/fabricated citations')\n",
        "\n",
        "print(f'Loading {MODEL_NAME} with eager attention...')\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "scorer = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map='auto',\n",
        "    attn_implementation='eager'  # Required for attention weights\n",
        ")\n",
        "if tokenizer.pad_token is None: tokenizer.pad_token = tokenizer.eos_token\n",
        "scorer.eval()\n",
        "NL = scorer.config.num_hidden_layers\n",
        "print(f'Ready: {NL} layers, VRAM: {torch.cuda.memory_allocated()/1e9:.1f} GB')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Y0PO-MPV42b",
        "outputId": "d06ba9ab-0702-418d-ceff-afd561afd4d3"
      },
      "outputs": [],
      "source": [
        "# --- HOOKS: separate attention vs FFN pathways ---\n",
        "activation_cache = {}\n",
        "def get_activation(name):\n",
        "    def hook(model, input, output):\n",
        "        activation_cache[name] = output[0].detach()\n",
        "    return hook\n",
        "\n",
        "for i in range(NL):\n",
        "    scorer.model.layers[i].post_attention_layernorm.register_forward_hook(get_activation(f'layer_{i}_mid'))\n",
        "\n",
        "def find_cite_toks(essay, tok, ml):\n",
        "    txt = essay['prompt'] + essay['response']\n",
        "    enc = tok(txt, return_tensors='pt', return_offsets_mapping=True, truncation=True, max_length=ml)\n",
        "    offs = enc.offset_mapping[0].tolist()\n",
        "    plen = len(tok(essay['prompt'], return_tensors='pt').input_ids[0])\n",
        "    res = []\n",
        "    for c in essay['citations']:\n",
        "        if c.get('label') not in ('real','fabricated'): continue\n",
        "        cs = c['start_pos'] + len(essay['prompt'])\n",
        "        for i,(ts,te) in enumerate(offs):\n",
        "            if ts <= cs < te: res.append((c,i)); break\n",
        "    return enc, plen, res\n",
        "\n",
        "all_scores = []\n",
        "for essay in tqdm(scorable, desc='Scoring'):\n",
        "    enc, plen, cps = find_cite_toks(essay, tokenizer, SCORE_MAX_LEN)\n",
        "    if not cps: continue\n",
        "\n",
        "    try:\n",
        "        activation_cache = {}\n",
        "        with torch.no_grad():\n",
        "            out = scorer(input_ids=enc.input_ids.cuda(),\n",
        "                         output_hidden_states=True, output_attentions=True, return_dict=True)\n",
        "        hs = tuple(h.cpu() for h in out.hidden_states)\n",
        "        attn = torch.stack(out.attentions).squeeze(1).cpu()\n",
        "        del out; torch.cuda.empty_cache()\n",
        "    except Exception as e:\n",
        "        print(f'  Error {essay[\"prompt_id\"]}: {e}'); continue\n",
        "\n",
        "    for c, tp in cps:\n",
        "        ics, pas, pfs, bas = [], [], [], []\n",
        "        for l in range(NL):\n",
        "            pre  = hs[l][0, tp].cuda()\n",
        "            mid  = activation_cache[f'layer_{l}_mid'][0, tp].cuda()\n",
        "            post = hs[l+1][0, tp].cuda()\n",
        "\n",
        "            v_attn = mid - pre\n",
        "            v_ffn  = post - mid\n",
        "\n",
        "            # ICS: attention-weighted context similarity\n",
        "            if l < attn.shape[0]:\n",
        "                a = attn[l, :, tp, plen:tp].cuda()\n",
        "                if a.shape[-1] > 0:\n",
        "                    a = a / (a.sum(dim=-1, keepdim=True) + 1e-9)\n",
        "                    a = a.to(dtype=torch.float16)\n",
        "                    ctx = torch.matmul(a, hs[l+1][0, plen:tp].cuda())\n",
        "                    ics_val = F.cosine_similarity(ctx, post.unsqueeze(0).expand_as(ctx), dim=-1).mean().item()\n",
        "                else: ics_val = 0.0\n",
        "            else: ics_val = 0.0\n",
        "            ics.append(ics_val)\n",
        "\n",
        "            # PAS: cosine sim between attn and FFN pathways\n",
        "            if torch.norm(v_attn) > 1e-6 and torch.norm(v_ffn) > 1e-6:\n",
        "                pas.append(F.cosine_similarity(v_attn.unsqueeze(0), v_ffn.unsqueeze(0)).item())\n",
        "            else:\n",
        "                pas.append(0.0)\n",
        "\n",
        "            # PFS: FFN force magnitude\n",
        "            pfs.append(torch.norm(v_ffn).item())\n",
        "\n",
        "            # BAS: attention to BOS\n",
        "            if l < attn.shape[0]:\n",
        "                bas.append(attn[l, :, tp, 0].mean().item())\n",
        "            else: bas.append(0.0)\n",
        "\n",
        "        all_scores.append({\n",
        "            'ics_scores': ics, 'ics_mean': sum(ics)/len(ics), 'ics_final': ics[-1],\n",
        "            'pas_scores': pas, 'pas_mean': sum(pas)/len(pas), 'pas_final': pas[-1],\n",
        "            'pfs_scores': pfs, 'pfs_mean': sum(pfs)/len(pfs), 'pfs_final': pfs[-1],\n",
        "            'bas_scores': bas, 'bas_mean': sum(bas)/len(bas),\n",
        "            'prompt_id': essay['prompt_id'], 'domain': essay['domain'],\n",
        "            'label': c.get('label','?'), 'citation': c['raw_text'],\n",
        "        })\n",
        "    del hs, attn; torch.cuda.empty_cache()\n",
        "\n",
        "print(f'\\nScored {len(all_scores)} citations')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ykyZK-M9V42e"
      },
      "source": [
        "---\n",
        "## 7. Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 774
        },
        "id": "q8niuXiDV42l",
        "outputId": "b088a79f-aa57-4310-9521-047289020694"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "if all_scores:\n",
        "    df = pd.DataFrame(all_scores)\n",
        "    cols = [c for c in ['prompt_id','domain','citation','label','ics_mean','pas_mean','pfs_mean','bas_mean'] if c in df.columns]\n",
        "    display(df[cols].round(4))\n",
        "    print('\\n=== Averages by Label ===')\n",
        "    num = [c for c in ['ics_mean','pas_mean','pfs_mean','bas_mean'] if c in df.columns]\n",
        "    if len(df['label'].unique()) > 1:\n",
        "        print(df.groupby('label')[num].mean().round(4).to_string())\n",
        "    else:\n",
        "        print(f'All: {df[\"label\"].iloc[0]}')\n",
        "        print(df[num].describe().round(4).to_string())\n",
        "else:\n",
        "    print('No citations scored.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 532
        },
        "id": "zB179OtCV42s",
        "outputId": "e104dd4a-9b2e-40dd-9f4c-89c30d622b2d"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "if all_scores and len(df['label'].unique()) > 1:\n",
        "    fig, axes = plt.subplots(1, 4, figsize=(20, 5))\n",
        "    for ax, (key, title) in zip(axes, [('ics_scores','ICS'),('pas_scores','PAS'),('pfs_scores','PFS'),('bas_scores','BAS')]):\n",
        "        for label in sorted(df['label'].unique()):\n",
        "            sub = df[df['label']==label]\n",
        "            means = []\n",
        "            for l in range(NL):\n",
        "                vals = [s[l] for s in sub[key] if len(s)>l]\n",
        "                means.append(sum(vals)/len(vals) if vals else 0)\n",
        "            color = {'real':'green','fabricated':'red'}.get(label,'gray')\n",
        "            style = '--' if label == 'fabricated' else '-'\n",
        "            ax.plot(range(NL), means, label=label, color=color, alpha=0.8, lw=2, linestyle=style)\n",
        "        ax.set_title(title, fontsize=14, fontweight='bold')\n",
        "        ax.set_xlabel('Layer'); ax.legend(); ax.grid(True, alpha=0.3)\n",
        "    plt.suptitle(f'Mechanistic Signatures — {MODEL_NAME.split(\"/\")[-1]}', fontsize=16, fontweight='bold')\n",
        "    plt.tight_layout()\n",
        "    os.makedirs('data/results', exist_ok=True)\n",
        "    plt.savefig('data/results/layer_scores.png', dpi=150, bbox_inches='tight')\n",
        "    plt.show()\n",
        "else:\n",
        "    print('Need both real and fabricated citations for comparison plots')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rHp8yK4xV42y",
        "outputId": "6ceacece-dd39-4be4-eac6-5e3a3f841dbd"
      },
      "outputs": [],
      "source": [
        "# Save scores & copy to Google Drive\n",
        "import shutil\n",
        "os.makedirs('data/results', exist_ok=True)\n",
        "sc_out = [{k: ([float(x) for x in v] if isinstance(v, list) else v) for k, v in s.items()} for s in all_scores]\n",
        "scores_file = f'data/results/{MODEL_NAME.split(\"/\")[-1]}_scores.json'\n",
        "with open(scores_file, 'w') as f: json.dump(sc_out, f, indent=2)\n",
        "with open(ESSAY_FILE, 'w') as f:\n",
        "    for e in essays: f.write(json.dumps(e) + '\\n')\n",
        "\n",
        "# Copy everything to Drive\n",
        "for src_file in [scores_file, ESSAY_FILE, 'data/results/layer_scores.png']:\n",
        "    if os.path.exists(src_file):\n",
        "        shutil.copy2(src_file, f'{DRIVE_BACKUP}/{os.path.basename(src_file)}')\n",
        "print(f'All results saved to Drive: {DRIVE_BACKUP}/')\n",
        "print('Download from Google Drive > MyDrive > soppery_results')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u9lG-dQlV43F"
      },
      "source": [
        "---\n",
        "## 8. Interpreting the Graphs\n",
        "\n",
        "Here's what to look for in each plot:\n",
        "\n",
        "### ICS (Internal Consistency Score)\n",
        "- Measures how much the citation \"fits\" with the surrounding essay text\n",
        "- **Expected:** Real citations should have **higher ICS** (more consistent with context) in later layers where semantic meaning is encoded\n",
        "- **If real ≈ fabricated:** The model generates both types with similar internal consistency — it doesn't \"know\" the citation is fake at the representation level\n",
        "\n",
        "### PAS (Pathway Alignment Score)\n",
        "- Measures internal agreement between attention (what the model reads) and FFN (what it recalls from parameters)\n",
        "- **Expected:** Fabricated citations should show **higher PAS** (more internal conflict) in middle layers where factual knowledge is stored\n",
        "- **If both spike in early layers:** The early layers are doing general token processing — the interesting signal is in **layers 10–20** for a 28-layer model\n",
        "\n",
        "### PFS (Parametric Force Score)  \n",
        "- Raw magnitude of the hidden state update — how hard the FFN is \"pushing\"\n",
        "- **Expected:** Fabricated citations need stronger parametric force since they aren't grounded in real knowledge\n",
        "- **If similar:** The model applies similar computational effort to both types\n",
        "\n",
        "### BAS (BOS Attention Score)\n",
        "- How much the citation token attends to the BOS (beginning-of-sequence) token\n",
        "- **Expected:** Higher BAS = model is relying more on parametric memory. Fabricated citations may show **different BAS patterns** in middle/late layers\n",
        "- **Bell curve pattern (peaking layers 5-10):** Normal — BOS acts as a \"memory sink\" in transformer models\n",
        "\n",
        "### What if curves overlap heavily?\n",
        "This means the 3B model's internal representations don't clearly distinguish real from fabricated citations. Possible reasons:\n",
        "1. The model is too small to have separable mechanistic signatures\n",
        "2. More data (more essays) is needed for the averages to separate\n",
        "3. The zero-shot setting makes it harder (no external context to contrast against)\n",
        "4. Different layer ranges or per-head analysis might reveal subtler signals"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "01cfd4b58b424150a16f9c03141ccfb5": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "05a11b9ed95049a3bae105625198156a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_18a1659ce2d44fe7bce72d3034265b0f",
              "IPY_MODEL_7d6fdf10e1ef48fb9c2547f2b45e8a8b",
              "IPY_MODEL_caf1204314ca4fa794481bcb75cc8419"
            ],
            "layout": "IPY_MODEL_60e0d2f1406e4ef194e03fad9728d00a"
          }
        },
        "11b45aa892cd41eab33b978a1072c2ca": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "18a1659ce2d44fe7bce72d3034265b0f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_11b45aa892cd41eab33b978a1072c2ca",
            "placeholder": "​",
            "style": "IPY_MODEL_2804673dd6eb4128a45d763eb3b588b4",
            "value": "Loading weights: 100%"
          }
        },
        "2804673dd6eb4128a45d763eb3b588b4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2bea0ee09c554802ba79f72671af0b67": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2e423849c8a04940a0d296434ff29df6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e2eeca48b1c945c68fe3ad0929dc5c13",
            "max": 254,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8deba9c86af44d8ab5a3fa36b5ae4220",
            "value": 254
          }
        },
        "60e0d2f1406e4ef194e03fad9728d00a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "63034f07791a466b8ced2e5de8425c93": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6c17268083044faa9be007fcdbf81f8f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6f6e67ba1a8e4dfba66c071b90176b82": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "77af65392a2145cab3a5296c1011d22e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7f839efd9a87457e9253e10c294b3237",
            "placeholder": "​",
            "style": "IPY_MODEL_e35313ada2964a5f93596fd7c5188892",
            "value": "Loading weights: 100%"
          }
        },
        "7d6fdf10e1ef48fb9c2547f2b45e8a8b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_01cfd4b58b424150a16f9c03141ccfb5",
            "max": 254,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6f6e67ba1a8e4dfba66c071b90176b82",
            "value": 254
          }
        },
        "7f839efd9a87457e9253e10c294b3237": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8deba9c86af44d8ab5a3fa36b5ae4220": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9388dd157ed7414aad44ae804c67fa36": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b09c5a9530524af681d076f478f77670": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_77af65392a2145cab3a5296c1011d22e",
              "IPY_MODEL_2e423849c8a04940a0d296434ff29df6",
              "IPY_MODEL_cdcde1814359405994676994bf6f7126"
            ],
            "layout": "IPY_MODEL_6c17268083044faa9be007fcdbf81f8f"
          }
        },
        "b6827ba888f1484d888c091bb3b0c71e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "caf1204314ca4fa794481bcb75cc8419": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_63034f07791a466b8ced2e5de8425c93",
            "placeholder": "​",
            "style": "IPY_MODEL_9388dd157ed7414aad44ae804c67fa36",
            "value": " 254/254 [00:25&lt;00:00, 10.34it/s, Materializing param=model.norm.weight]"
          }
        },
        "cdcde1814359405994676994bf6f7126": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2bea0ee09c554802ba79f72671af0b67",
            "placeholder": "​",
            "style": "IPY_MODEL_b6827ba888f1484d888c091bb3b0c71e",
            "value": " 254/254 [00:29&lt;00:00,  9.21it/s, Materializing param=model.norm.weight]"
          }
        },
        "e2eeca48b1c945c68fe3ad0929dc5c13": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e35313ada2964a5f93596fd7c5188892": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}